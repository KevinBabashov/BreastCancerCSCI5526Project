{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eccc2088",
   "metadata": {},
   "source": [
    "\n",
    "# Breast Cancer CNN â€” **PyTorch** (Modernized)\n",
    "\n",
    "**Converted on:** 2025-10-17 01:43:26  \n",
    "**Why PyTorch?** It's the current industry standard in research and production, with flexible APIs and fast GPU support.\n",
    "\n",
    "### What's new vs. the original TensorFlow/Keras notebook\n",
    "- Uses **PyTorch** (`torch`, `torchvision`).\n",
    "- Adds **Batch Normalization** after conv layers.\n",
    "- Uses **AdamW** optimizer (often better generalization than plain Adam).\n",
    "- Includes **Early Stopping** and **Model Checkpointing**.\n",
    "- Supports **GPU** automatically when available.\n",
    "- Optional **Automatic Mixed Precision (AMP)** for faster training on GPU.\n",
    "- Same dataset and intent; structure is comparable for apples-to-apples results.\n",
    "\n",
    "> ðŸ’¡ Each section below is self-contained and **fully editable**. You can tweak paths, transforms, model layers, and training hyperparameters cell-by-cell.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e604f1d",
   "metadata": {},
   "source": [
    "## 1) Environment & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3b0e22e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "CUDA available: False\n",
      "Using AMP: False\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Editable: core imports\n",
    "import os\n",
    "import random\n",
    "import json\n",
    "import math\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import make_grid\n",
    "\n",
    "# Visualization & metrics\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import itertools\n",
    "\n",
    "# Reproducibility\n",
    "def set_seed(seed: int = 42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "SEED = 42\n",
    "set_seed(SEED)\n",
    "\n",
    "# Device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "\n",
    "# Optional: enable TF32 (Ampere+ GPUs) for faster matmul/convs while preserving accuracy\n",
    "if torch.cuda.is_available():\n",
    "    try:\n",
    "        torch.backends.cuda.matmul.allow_tf32 = True\n",
    "        torch.backends.cudnn.allow_tf32 = True\n",
    "        print(\"Enabled TF32 on supported GPUs.\")\n",
    "    except Exception as e:\n",
    "        print(\"TF32 not enabled:\", e)\n",
    "\n",
    "# Optional Mixed Precision\n",
    "USE_AMP = True if torch.cuda.is_available() else False\n",
    "print(\"Using AMP:\", USE_AMP)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5908d98c",
   "metadata": {},
   "source": [
    "\n",
    "## 2) Data Paths & (Optional) Kaggle Download\n",
    "\n",
    "Update these paths to point to your **Kaggle dataset**.  \n",
    "If your images are arranged like `root/class_x/...jpg` and `root/class_y/...jpg`, you can use `ImageFolder` directly.\n",
    "\n",
    "- If you already have the dataset locally, set `DATA_ROOT` to that folder.\n",
    "- If you'd like to **download from Kaggle** in-notebook, fill in the `COMPETITION_OR_DATASET` and ensure your `kaggle.json` is configured.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2860edf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# === Editable ===\n",
    "# If the dataset is already downloaded and extracted locally, point DATA_ROOT to it.\n",
    "# Example structure expected by ImageFolder:\n",
    "# DATA_ROOT/\n",
    "#   â”œâ”€â”€ train/\n",
    "#   â”‚    â”œâ”€â”€ benign/ *.png|*.jpg\n",
    "#   â”‚    â””â”€â”€ malignant/ *.png|*.jpg\n",
    "#   â””â”€â”€ val/   (optional; if not present we'll split from train)\n",
    "DATA_ROOT = Path(\"/path/to/your/breast_cancer_dataset\")  # <-- EDIT THIS\n",
    "TRAIN_DIR = DATA_ROOT / \"train\"\n",
    "VAL_DIR = DATA_ROOT / \"val\"    # optional\n",
    "TEST_DIR = DATA_ROOT / \"test\"  # optional\n",
    "\n",
    "# (Optional) Kaggle dataset/competition settings â€” use ONLY if you want to download here\n",
    "# Example: \"paultimothymooney/breast-histopathology-images\"\n",
    "KAGGLE_DATASET = \"\"  # e.g., \"paultimothymooney/breast-histopathology-images\"\n",
    "KAGGLE_COMPETITION = \"\"  # leave empty unless it's a competition\n",
    "KAGGLE_DOWNLOAD_DIR = Path(\"/mnt/data/kaggle_download\")  # safe default inside this environment\n",
    "\n",
    "def kaggle_download(dataset:str=\"\", competition:str=\"\", out_dir:Path=KAGGLE_DOWNLOAD_DIR):\n",
    "    if not dataset and not competition:\n",
    "        print(\"No Kaggle dataset/competition specified. Skipping download.\")\n",
    "        return\n",
    "\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    try:\n",
    "        import subprocess, zipfile, glob\n",
    "\n",
    "        if dataset:\n",
    "            print(f\"Downloading Kaggle dataset: {dataset}\")\n",
    "            subprocess.check_call([\"kaggle\", \"datasets\", \"download\", \"-d\", dataset, \"-p\", str(out_dir)])\n",
    "        elif competition:\n",
    "            print(f\"Downloading Kaggle competition: {competition}\")\n",
    "            subprocess.check_call([\"kaggle\", \"competitions\", \"download\", \"-c\", competition, \"-p\", str(out_dir)])\n",
    "\n",
    "        # Unzip all zips found\n",
    "        for z in out_dir.glob(\"*.zip\"):\n",
    "            print(f\"Unzipping {z.name} ...\")\n",
    "            with zipfile.ZipFile(z, 'r') as zip_ref:\n",
    "                zip_ref.extractall(out_dir)\n",
    "        print(\"Kaggle download and unzip complete. Inspect files and set DATA_ROOT accordingly.\")\n",
    "    except Exception as e:\n",
    "        print(\"Kaggle download failed or kaggle CLI not available:\", e)\n",
    "\n",
    "# Uncomment to trigger a download (if needed):\n",
    "# kaggle_download(dataset=KAGGLE_DATASET, competition=KAGGLE_COMPETITION, out_dir=KAGGLE_DOWNLOAD_DIR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77ed1f5e",
   "metadata": {},
   "source": [
    "## 3) Transforms, Datasets, and Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1e2d7dd9",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "TRAIN_DIR not found: /path/to/your/breast_cancer_dataset/train. Please set DATA_ROOT correctly.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 23\u001b[0m\n\u001b[1;32m      8\u001b[0m train_tfms \u001b[38;5;241m=\u001b[39m transforms\u001b[38;5;241m.\u001b[39mCompose([\n\u001b[1;32m      9\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mResize((IMG_SIZE, IMG_SIZE)),\n\u001b[1;32m     10\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mRandomHorizontalFlip(p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     14\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mNormalize(mean\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m0.5\u001b[39m, \u001b[38;5;241m0.5\u001b[39m, \u001b[38;5;241m0.5\u001b[39m], std\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m0.5\u001b[39m, \u001b[38;5;241m0.5\u001b[39m, \u001b[38;5;241m0.5\u001b[39m]),\n\u001b[1;32m     15\u001b[0m ])\n\u001b[1;32m     17\u001b[0m eval_tfms \u001b[38;5;241m=\u001b[39m transforms\u001b[38;5;241m.\u001b[39mCompose([\n\u001b[1;32m     18\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mResize((IMG_SIZE, IMG_SIZE)),\n\u001b[1;32m     19\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mToTensor(),\n\u001b[1;32m     20\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mNormalize(mean\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m0.5\u001b[39m, \u001b[38;5;241m0.5\u001b[39m, \u001b[38;5;241m0.5\u001b[39m], std\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m0.5\u001b[39m, \u001b[38;5;241m0.5\u001b[39m, \u001b[38;5;241m0.5\u001b[39m]),\n\u001b[1;32m     21\u001b[0m ])\n\u001b[0;32m---> 23\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m TRAIN_DIR\u001b[38;5;241m.\u001b[39mexists(), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTRAIN_DIR not found: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mTRAIN_DIR\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Please set DATA_ROOT correctly.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     24\u001b[0m train_dataset_full \u001b[38;5;241m=\u001b[39m datasets\u001b[38;5;241m.\u001b[39mImageFolder(root\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mstr\u001b[39m(TRAIN_DIR), transform\u001b[38;5;241m=\u001b[39mtrain_tfms)\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m VAL_DIR\u001b[38;5;241m.\u001b[39mexists():\n",
      "\u001b[0;31mAssertionError\u001b[0m: TRAIN_DIR not found: /path/to/your/breast_cancer_dataset/train. Please set DATA_ROOT correctly."
     ]
    }
   ],
   "source": [
    "\n",
    "# === Editable ===\n",
    "IMG_SIZE = 224   # Common choice for many CNNs\n",
    "BATCH_SIZE = 32\n",
    "VAL_SPLIT = 0.2  # used only if VAL_DIR doesn't exist\n",
    "NUM_WORKERS = os.cpu_count() if os.cpu_count() else 2\n",
    "\n",
    "# Typical histopathology augmentations can be mild; adjust as needed\n",
    "train_tfms = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomVerticalFlip(p=0.5),\n",
    "    transforms.RandomRotation(degrees=10),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
    "])\n",
    "\n",
    "eval_tfms = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
    "])\n",
    "\n",
    "assert TRAIN_DIR.exists(), f\"TRAIN_DIR not found: {TRAIN_DIR}. Please set DATA_ROOT correctly.\"\n",
    "train_dataset_full = datasets.ImageFolder(root=str(TRAIN_DIR), transform=train_tfms)\n",
    "\n",
    "if VAL_DIR.exists():\n",
    "    val_dataset = datasets.ImageFolder(root=str(VAL_DIR), transform=eval_tfms)\n",
    "    train_dataset = train_dataset_full\n",
    "else:\n",
    "    # Split from train\n",
    "    val_len = int(len(train_dataset_full) * VAL_SPLIT)\n",
    "    train_len = len(train_dataset_full) - val_len\n",
    "    train_dataset, val_dataset = random_split(train_dataset_full, [train_len, val_len], generator=torch.Generator().manual_seed(SEED))\n",
    "\n",
    "if TEST_DIR.exists():\n",
    "    test_dataset = datasets.ImageFolder(root=str(TEST_DIR), transform=eval_tfms)\n",
    "else:\n",
    "    test_dataset = None\n",
    "\n",
    "class_names = train_dataset_full.classes if hasattr(train_dataset_full, \"classes\") else None\n",
    "print(\"Classes:\", class_names)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS, pin_memory=torch.cuda.is_available())\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=torch.cuda.is_available())\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=torch.cuda.is_available()) if test_dataset else None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f41f83ad",
   "metadata": {},
   "source": [
    "## 4) Model Definition (Slightly Modernized CNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb643dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# === Editable ===\n",
    "# A compact CNN with BatchNorm and Dropout. Feel free to tweak channels/depth.\n",
    "class BreastCancerCNN(nn.Module):\n",
    "    def __init__(self, num_classes: int):\n",
    "        super().__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, padding=1),  # -> 32x224x224\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2),                              # -> 32x112x112\n",
    "\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),  # -> 64x112x112\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2),                              # -> 64x56x56\n",
    "\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1), # -> 128x56x56\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2),                              # -> 128x28x28\n",
    "\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1),# -> 256x28x28\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2),                              # -> 256x14x14\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(p=0.3),\n",
    "            nn.Linear(256 * 14 * 14, 512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(p=0.4),\n",
    "            nn.Linear(512, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "num_classes = len(class_names) if class_names else 2\n",
    "model = BreastCancerCNN(num_classes=num_classes).to(device)\n",
    "\n",
    "# Optional: better init\n",
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Conv2d):\n",
    "        nn.init.kaiming_normal_(m.weight, nonlinearity='relu')\n",
    "        if m.bias is not None:\n",
    "            nn.init.constant_(m.bias, 0.0)\n",
    "    elif isinstance(m, nn.Linear):\n",
    "        nn.init.kaiming_normal_(m.weight, nonlinearity='relu')\n",
    "        if m.bias is not None:\n",
    "            nn.init.constant_(m.bias, 0.0)\n",
    "\n",
    "model.apply(init_weights)\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fcf007c",
   "metadata": {},
   "source": [
    "## 5) Training Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd0304f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# === Editable ===\n",
    "EPOCHS = 15\n",
    "LEARNING_RATE = 1e-3\n",
    "WEIGHT_DECAY = 1e-4\n",
    "PATIENCE = 5  # early stopping patience\n",
    "CHECKPOINT_DIR = Path(\"./checkpoints\")\n",
    "CHECKPOINT_DIR.mkdir(exist_ok=True, parents=True)\n",
    "CKPT_PATH = CHECKPOINT_DIR / \"best_model.pt\"\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=USE_AMP)\n",
    "\n",
    "def accuracy_from_logits(logits, targets):\n",
    "    preds = torch.argmax(logits, dim=1)\n",
    "    return (preds == targets).float().mean().item()\n",
    "\n",
    "def train_one_epoch(model, loader, optimizer, criterion, scaler, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    running_acc = 0.0\n",
    "    n = 0\n",
    "    for imgs, labels in loader:\n",
    "        imgs, labels = imgs.to(device, non_blocking=True), labels.to(device, non_blocking=True)\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        with torch.cuda.amp.autocast(enabled=USE_AMP):\n",
    "            logits = model(imgs)\n",
    "            loss = criterion(logits, labels)\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        batch_size = imgs.size(0)\n",
    "        running_loss += loss.item() * batch_size\n",
    "        running_acc  += accuracy_from_logits(logits, labels) * batch_size\n",
    "        n += batch_size\n",
    "\n",
    "    return running_loss / n, running_acc / n\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    running_acc = 0.0\n",
    "    n = 0\n",
    "    for imgs, labels in loader:\n",
    "        imgs, labels = imgs.to(device, non_blocking=True), labels.to(device, non_blocking=True)\n",
    "        with torch.cuda.amp.autocast(enabled=USE_AMP):\n",
    "            logits = model(imgs)\n",
    "            loss = criterion(logits, labels)\n",
    "\n",
    "        batch_size = imgs.size(0)\n",
    "        running_loss += loss.item() * batch_size\n",
    "        running_acc  += accuracy_from_logits(logits, labels) * batch_size\n",
    "        n += batch_size\n",
    "\n",
    "    return running_loss / n, running_acc / n\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ab87087",
   "metadata": {},
   "source": [
    "## 6) Train (with Early Stopping & Checkpointing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d742fc52",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "best_val_loss = float(\"inf\")\n",
    "epochs_no_improve = 0\n",
    "history = {\"train_loss\": [], \"val_loss\": [], \"train_acc\": [], \"val_acc\": []}\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    t0 = time.time()\n",
    "    train_loss, train_acc = train_one_epoch(model, train_loader, optimizer, criterion, scaler, device)\n",
    "    val_loss, val_acc = evaluate(model, val_loader, criterion, device)\n",
    "    dt = time.time() - t0\n",
    "\n",
    "    history[\"train_loss\"].append(train_loss)\n",
    "    history[\"val_loss\"].append(val_loss)\n",
    "    history[\"train_acc\"].append(train_acc)\n",
    "    history[\"val_acc\"].append(val_acc)\n",
    "\n",
    "    print(f\"Epoch {epoch:02d}/{EPOCHS} | \"\n",
    "          f\"train_loss={train_loss:.4f} val_loss={val_loss:.4f} | \"\n",
    "          f\"train_acc={train_acc:.4f} val_acc={val_acc:.4f} | {dt:.1f}s\")\n",
    "\n",
    "    # Early Stopping & Checkpoint\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        epochs_no_improve = 0\n",
    "        torch.save({\"model_state\": model.state_dict(),\n",
    "                    \"optimizer_state\": optimizer.state_dict(),\n",
    "                    \"epoch\": epoch,\n",
    "                    \"val_loss\": val_loss,\n",
    "                    \"class_names\": class_names}, CKPT_PATH)\n",
    "        print(f\"  â†³ Saved new best checkpoint to {CKPT_PATH}\")\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "        if epochs_no_improve >= PATIENCE:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "# Load best weights for evaluation\n",
    "if CKPT_PATH.exists():\n",
    "    ckpt = torch.load(CKPT_PATH, map_location=device)\n",
    "    model.load_state_dict(ckpt[\"model_state\"])\n",
    "    print(f\"Loaded best model from epoch {ckpt.get('epoch')} with val_loss={ckpt.get('val_loss'):.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c3547f",
   "metadata": {},
   "source": [
    "## 7) Loss & Accuracy Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c575eea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Plot training curves\n",
    "epochs_range = range(1, len(history[\"train_loss\"]) + 1)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(epochs_range, history[\"train_loss\"], label=\"Train Loss\")\n",
    "plt.plot(epochs_range, history[\"val_loss\"], label=\"Val Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Loss over Epochs\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(epochs_range, history[\"train_acc\"], label=\"Train Acc\")\n",
    "plt.plot(epochs_range, history[\"val_acc\"], label=\"Val Acc\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Accuracy over Epochs\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24bdab0b",
   "metadata": {},
   "source": [
    "## 8) Evaluation: Confusion Matrix & Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5005597e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@torch.no_grad()\n",
    "def get_all_preds_targets(model, loader, device):\n",
    "    model.eval()\n",
    "    all_preds, all_targets = [], []\n",
    "    for imgs, labels in loader:\n",
    "        imgs = imgs.to(device, non_blocking=True)\n",
    "        logits = model(imgs)\n",
    "        preds = torch.argmax(logits, dim=1).cpu().numpy().tolist()\n",
    "        all_preds.extend(preds)\n",
    "        all_targets.extend(labels.numpy().tolist())\n",
    "    return np.array(all_preds), np.array(all_targets)\n",
    "\n",
    "eval_loader = test_loader if test_loader is not None else val_loader\n",
    "preds, targets = get_all_preds_targets(model, eval_loader, device)\n",
    "\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(targets, preds, target_names=class_names if class_names else None, digits=4))\n",
    "\n",
    "cm = confusion_matrix(targets, preds)\n",
    "def plot_confusion_matrix(cm, classes, normalize=False, title='Confusion matrix'):\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    plt.figure()\n",
    "    plt.imshow(cm, interpolation='nearest')\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_confusion_matrix(cm, class_names if class_names else [str(i) for i in range(num_classes)], normalize=False, title='Confusion Matrix')\n",
    "plot_confusion_matrix(cm, class_names if class_names else [str(i) for i in range(num_classes)], normalize=True, title='Confusion Matrix (Normalized)')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a27bd46",
   "metadata": {},
   "source": [
    "## 9) Inference Helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f2b308",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from PIL import Image\n",
    "\n",
    "@torch.no_grad()\n",
    "def predict_image(model, image_path: str, transform, device):\n",
    "    model.eval()\n",
    "    img = Image.open(image_path).convert(\"RGB\")\n",
    "    x = transform(img).unsqueeze(0).to(device)\n",
    "    logits = model(x)\n",
    "    probs = F.softmax(logits, dim=1).cpu().numpy()[0]\n",
    "    pred_idx = int(np.argmax(probs))\n",
    "    return pred_idx, probs\n",
    "\n",
    "# Example usage (edit path):\n",
    "# img_path = \"/path/to/sample.jpg\"\n",
    "# idx, probs = predict_image(model, img_path, eval_tfms, device)\n",
    "# print(\"Predicted:\", class_names[idx], \"probs=\", probs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c57bb1ed",
   "metadata": {},
   "source": [
    "## 10) Save / Load Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c0ffd1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "FINAL_MODEL_PATH = Path(\"./breast_cancer_cnn_pytorch.pt\")\n",
    "\n",
    "def save_final(model, path=FINAL_MODEL_PATH):\n",
    "    torch.save({\"model_state\": model.state_dict(),\n",
    "                \"class_names\": class_names}, path)\n",
    "    print(f\"Saved model to {path}\")\n",
    "\n",
    "def load_final(path=FINAL_MODEL_PATH, device=device):\n",
    "    ckpt = torch.load(path, map_location=device)\n",
    "    model = BreastCancerCNN(num_classes=len(ckpt[\"class_names\"])).to(device)\n",
    "    model.load_state_dict(ckpt[\"model_state\"])\n",
    "    return model, ckpt[\"class_names\"]\n",
    "\n",
    "# Example:\n",
    "# save_final(model, FINAL_MODEL_PATH)\n",
    "# loaded_model, loaded_classes = load_final(FINAL_MODEL_PATH, device=device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b79b394f",
   "metadata": {},
   "source": [
    "\n",
    "## 11) Notes on Reproducibility & Matching Results\n",
    "\n",
    "- Seeding (`SEED=42`) and disabling CuDNN nondeterminism are enabled for **stable results**.\n",
    "- Minor differences between TensorFlow and PyTorch (initialization, data order, AMP) can cause small metric variations.\n",
    "- To get as close as possible to the original:\n",
    "  - Use identical image sizes, augmentations, splits, and batch size.\n",
    "  - Keep learning rate, epochs, and optimizer comparable (we use **AdamW** with modest weight decay for better generalization).\n",
    "  - If exact parity is required, set `USE_AMP = False` and consider removing BatchNorm or Dropout changes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dec9dd6",
   "metadata": {},
   "source": [
    "## 12) Sanity Check: Visualize a Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36b324ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def show_batch(dl):\n",
    "    imgs, labels = next(iter(dl))\n",
    "    grid = make_grid(imgs[:16], nrow=8, padding=2, normalize=True)\n",
    "    plt.figure(figsize=(10,5))\n",
    "    plt.imshow(grid.permute(1,2,0))\n",
    "    plt.axis('off')\n",
    "    if hasattr(dl.dataset, 'dataset') and hasattr(dl.dataset.dataset, 'classes'):\n",
    "        classes = dl.dataset.dataset.classes\n",
    "    elif hasattr(dl.dataset, 'classes'):\n",
    "        classes = dl.dataset.classes\n",
    "    else:\n",
    "        classes = class_names\n",
    "    plt.title(\"A few training samples\")\n",
    "    plt.show()\n",
    "\n",
    "# Uncomment to preview:\n",
    "# show_batch(train_loader)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
